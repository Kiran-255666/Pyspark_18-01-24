{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdOPkqaCh8-S"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "iOQf_w8tjCxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark configuration and set an app name\n",
        "spark = SparkSession.builder.appName('Word Count').getOrCreate()"
      ],
      "metadata": {
        "id": "WrXpqwHpiHr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the text file into an RDD\n",
        "lines = spark.sparkContext.textFile('sample_text.txt')"
      ],
      "metadata": {
        "id": "F4WnKZsfiHui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Use flatMap to split each line into words\n",
        "'''\n",
        "lines.flatMap(lambda line: line.split(\" \")) is used to split each line into words, resulting in an RDD called words.\n",
        "Map to Key-Value Pairs.\n",
        "'''\n",
        "words = lines.flatMap(lambda line: line.split(\" \"))"
      ],
      "metadata": {
        "id": "Yjx8chMviHxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Map each word to a key-value pair (word, 1)\n",
        "'''\n",
        "words.map(lambda word: (word, 1)) transforms each word into a key-value pair where the key is the word, and the value is 1.\n",
        "ReduceByKey to Sum Counts\n",
        "'''\n",
        "word_counts = words.map(lambda word: (word, 1))"
      ],
      "metadata": {
        "id": "mbF56KRXiH0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Use reduceByKey to sum the counts for each word\n",
        "'''\n",
        "word_counts.reduceByKey(lambda x, y: x + y) combines the counts for each word using the reduceByKey transformation.\n",
        "Sort the Result.\n",
        "'''\n",
        "word_count_sum = word_counts.reduceByKey(lambda x, y: x + y)"
      ],
      "metadata": {
        "id": "FBpgCzGziH21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Sort the result by count in descending order\n",
        "'''\n",
        "word_count_sum.sortBy(lambda x: x[1], ascending=False) sorts the result by count in descending order.\n",
        "'''\n",
        "\n",
        "sorted_word_count = word_count_sum.sortBy(lambda x: x[1], ascending=False)"
      ],
      "metadata": {
        "id": "Fpc2H3FoiH5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Collect and print the result\n",
        "'''\n",
        "collect() is used to retrieve the final result as a list, and the result is printed.\n",
        "'''\n",
        "\n",
        "result = sorted_word_count.collect()"
      ],
      "metadata": {
        "id": "OC_XAb8KiH8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for (word, count) in result:\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "3Pm--EH4iH-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Stop the SparkContext\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "XOErOG1HiFCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hyTfDx5ZlHm7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}